# ğŸ¦ Parser MediaWiki â€” Configurable API crawler for multiple Wikis by `NewtCode`

Parser MediaWiki is a Python MediaWiki API crawler designed to collect content from multiple wiki installations using separate settings per wiki (endpoints, namespaces, rate limits, auth, and export rules). It retrieves pages, revisions, categories, and related metadata and normalizes the output for downstream processing. Built for repeatable runs, it helps keep large-scale wiki data collection consistent across sources.

[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)

---

## ğŸ“– Overview

---

## ğŸ§© Features

---

## âš™ï¸ Requirements

- **Python 3.10+** (tested with Python 3.10, 3.11, 3.12, 3.13)
- Full type hint support with `from __future__ import annotations`

## ğŸ“¦ Dependencies

All other modules rely only on the Python Standard Library.

---

## ğŸš€ Getting Started

- [Installation Guide](INSTALL.md) â€” setup instructions and configuration details.

---

## ğŸ“‹ Development Notes

- [TODO list](TODO) â€” Planned improvements
- [CHANGELOG](CHANGELOG.md) â€” Version history
- [CONTRIBUTING](CONTRIBUTING.md) â€” Style and contribution rules

---

## ğŸªª License

- [COPYRIGHT](COPYRIGHT) â€” Copyright information for original and included materials.
- [LICENSE](LICENSE) â€” The license governing the use of this repository (MIT).
- [AUTHORS](AUTHORS) â€” List of contributors and credits for external resources.
